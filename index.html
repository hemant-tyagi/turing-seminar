<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>Theory and Algorithms for Data Science Seminar</title>

<meta name="viewport" content="width=device-width, initial-scale=1">

<script src="../jquery.js" type="text/javascript"></script>
<script src="../vallenato.js" type="text/javascript"></script>
<link rel="stylesheet" type="text/css" href="../style.css" media="screen" />

<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


</head>


<body>

<div id="main_container">

	<div id="header">

    </div>
	

	<div id="middle_box">
	</div>
		
<div class="center_content">
	
	<h1 class="cent"><b>Theory and Algorithms in Data Science</b></h1>
	
	    <div id="accordion-container">
			
		    	<p class = "tab">
				This seminar provides a forum for researchers working in foundational areas of Data Science
				(and related fields). All talks are held at the <a href="http://turing.ac.uk">Alan
				Turing Institute</a>. For a complete list of talks held in previous years, please click
				<a href = "past.html">here</a>.
		    	</p>
		    
		    	<p class = "tiny">
				The organizers are 
				<a href="http://www.stats.ox.ac.uk/~cucuring/">Mihai Cucuringu</a> and  
				<a href="http://people.maths.ox.ac.uk/nanda">Vidit Nanda</a>. 
				We represent the <a href = "https://www.turing.ac.uk/research_projects/low-dimensional-structure-data-models-analysis-algorithms/">
				Low-dimensional structure</a> as well as the <a href = "https://www.turing.ac.uk/research_projects/topology-and-geometry-data/">
				Topology and geometry</a> research interest groups within the Turing Institute. <b>If you'd like to speak in this seminar,
				please email Vidit at <a href="mailto:vnanda@turing.ac.uk">vnanda@turing.ac.uk</a></b>
		    </p>
		    
		    
					
<h1 class = "cent">Upcoming Talks</h1>
		   	    

		    
  
		  <h2  class="accordion-header" >16 May 2019: <b>Ioannis Kosmidis</b>, University of Warwick</h2>
               <div class="accordion-content">
		       			<img src ="http://www.ikosmidis.com/img/me.jpg" class="float-right" alt="Ioannis" height = "155">
						
						<p class="tab">
						<b> Speaker </b>: <a href = "http://ikosmidis.com">Ioannis Kosmidis</a>, University of Warwick
						<br>

						<b> Title </b>: Reduced-bias inference in regression modelling  
 						</p>
						
						<p class = "tab">

						<b>Abstract</b>:
						This talk focuses on a unified theoretical and algorithmic framework for reducing bias in the estimation of statistical models from a practitioners point of view.  The talk will briefly discuss how shortcomings of classical estimators can be overcome via reduction of bias, and provide a few illustrations for well-used statistical models with tractable likelihoods, including regression models with categorical responses and Beta regression.  New results will then be presented on the use of bias reduction methods when modelling longitudinal and clustered data with regression models. The substantial effect that the bias of the variance components can have on inference in the presence of heterogeneity motivates the application of the framework to deliver higher-order corrective methods for generalised linear mixed models. We ent will present the challenges in doing so along with resolutions stemming from current research.

		      				 </p>
		       
		       
						<p class= "tab">
							<b>Room and Time</b>: Augusta, 14h - 15h
		       				</p>
		    </div>
		     
						    
		 <h2  class="accordion-header" >11 Jun 2019: <b>Jia Chen</b>, University of York</h2>
               <div class="accordion-content">
		       TBA
		    </div> 
		    
		    
		   				    
		 <h2  class="accordion-header" >17 Jun 2019: <b>Renaud Iambiotte</b>, University of Oxford</h2>
               <div class="accordion-content">
		       TBA
		    </div> 
		    
		 	  <h2  class="accordion-header" >27 Jun 2019: <b>Jon Bloom</b>, Broad Institute</h2>
               <div class="accordion-content">
		       			<img src ="https://avatars3.githubusercontent.com/u/3201642?s=400&v=4" class="float-right" alt="Jon" height = "155">
						
						<p class="tab">
						<b> Speaker </b>: <a href = "http://math.mit.edu/~jbloom/">Jon Bloom</a>, Broad Institute (MIT/Harvard)
						<br>

						<b> Title </b>: Regularized linear autoencoders, the Morse theory of loss, and backprop in the brain  
 						</p>
						
						<p class = "tab">

						<b>Abstract</b>:
						When trained to minimize reconstruction error, a linear autoencoder (LAE) learns the subspace spanned by the top principal directions but cannot learn the principal directions themselves. In this talk, I'll explain how this observation led us to a satisfying conversation between numerical analysis, algebraic topology, random matrix theory, deep learning, and computational neuroscience. We'll see that an L2-regularized LAE learns the principal directions as the left singular vectors of the decoder, providing a simple and scalable PCA algorithm related to Hebbian learning. We'll use the lens of Morse theory to smoothly parameterize all LAE critical manifolds and the gradient trajectories between them; and see how algebra and probability theory provide principled foundations for ensemble learning in deep networks, while suggesting new algorithms. Finally, we'll give mathematical and in silico evidence for a simple, modular, and scalable resolution to the "weight transport problem" of backpropagation in the brain. Joint learning with Daniel Kunin, Aleksandrina Goeva, and Cotton Seed. To appear at ICML 2019.
		      				 </p>
		       
		       
						<p class= "tab">
							<b>Room and Time</b>: Augusta, 13h - 14h
		       				</p>
		    </div>
		     
							       
							       
<h1 class="cent">Recent Talks</h1>
		    
			    	
		
		 <h2  class="accordion-header" >09 May 2019: <b>Degui Li</b>, University of York</h2>
               <div class="accordion-content">
		       <img src ="https://www.york.ac.uk/media/mathematics/staffphotos/Degui%20Li.jpg" class="float-right" alt="Degui" height = "155">
						
						<p class="tab">
						<b> Speaker </b>: <a href = "https://www.york.ac.uk/maths/staff/degui-li/">Degui Li</a>, University of Bristol
						<br>

						<b> Title </b>: Detection of Multiple Structural Breaks in Large Covariance Matrices  
 						</p>
						
						<p class = "tab">

						<b>Abstract</b>:
						This talk is about multiple structural breaks in large covariance matrices of 
						high-dimensional time series variables satisfying the approximate factor model 
						structure. The breaks in the second-order structure of the common components 
						are due to sudden changes in either factor loadings or covariance of latent factors, 
						requiring appropriate transformation of the factor models to facilitate estimation 
						of the (transformed) common factors and factor loadings via the classical principal 
						component analysis. With the estimated factors and idiosyncratic errors, an 
						easy-to-implement CUSUM-based detection technique is introduced to consistently 
						estimate the location and number of breaks and correctly identify whether they 
						originate in the common or idiosyncratic error components. The algorithms of Wild 
						Binary Segmentation and Wild Sparsified Binary Segmentation are used to estimate the 
						breaks in the common and idiosyncratic error components, respectively. Under some mild conditions, the asymptotic properties of the proposed methodology are derived with near-optimal rates (up to a logarithmic factor) achieved for the estimated change points. Some numerical studies are conducted to examine the finite-sample performance of the developed method and its comparison with other existing approaches. This is a joint work with Yu-Ning Li (Zhejiang University) and Piotr Fryzlewicz (London School of Economics).
		       				</p>
		       
		       
						<p class= "tab">
							<b>Room and Time</b>: Augusta, 14h - 15h
		       				</p>
		    </div>
		    

		    
		     <h2  class="accordion-header" >30 Apr 2019: <b>Haeran Cho</b>, University of Bristol</h2>
               <div class="accordion-content">
		       
		       	<img src ="http://dbms.services.bris.ac.uk/media/user/187876/300.jpg" class="float-right" alt="Haeran" height = "155">
						
						<p class="tab">
						<b> Speaker </b>: <a href = "https://sites.google.com/view/haeran-cho/home">Haeran Cho</a>, University of Bristol
						<br>

						<b> Title </b>: Model selection in change-point problems

 						</p>
						
						<p class = "tab">

						<b>Abstract</b>: Lately, there has been a surge of interest in research for 
							computationally fast and statistically efficient methods for change-point 
							detection, as nonstationarities frequently observed in real-life datasets 
							are often attributed to structural breaks in the underlying stochastic 
							properties. In multiple change-point detection, model selection via 
							estimating the total number of change-points poses as a challenge, 
							particularly when the dimensionality of the data is large. In this 
							talk, I will address the model selection in change-point problems in 
							two different settings: when p = 1 where one can benefit from localised 
							application of an information criterion, and when p is large where the 
							change-point detection problem can be translated to that of detecting 
							pervasive and latent 'factors'.
		       				</p>
				
						<p class= "tab">
							<b>Room and Time</b>: Enigma, 1430h - 1530h
		       				</p>
		       
		    </div>
		    
		    
		    		
		   <h2  class="accordion-header" >26 Mar 2019: <b>David Barber</b>, University College London</h2>
               <div class="accordion-content">
						
						<img src ="http://www0.cs.ucl.ac.uk/people/photos/D.Barber.jpg" class="float-right" alt="David" height = "155">
						
						<p class="tab">
						<b> Speaker </b>: <a href = "http://www0.cs.ucl.ac.uk/people/D.Barber.html">David Barber</a>, University College London
						<br>

						<b> Title </b>: Spread Divergences
 

						</p>
						
						<p class = "tab">

						<b>Abstract</b>: For distributions p and q with different support, the divergence D(p||q) may not exist. We define a spread divergence on modified p and q and describe sufficient conditions for the existence of such a divergence. We give examples of using a spread divergence to train implicit generative models, including linear models (Principal Components Analysis and Independent Components Analysis) and non-linear models (Deep Generative Networks). We show how a general privacy preserving machine learning mechanism is a natural application of the spread divergence.
						</p>      
				
						<p class= "tab">
							<b>Room and Time</b>: Augusta, 14h - 15h
		       				</p>	       
			</div> 
		    
		    
		
		    
		    
		    
		    
		    	    
		    	<h2  class="accordion-header" >12 Mar 2019: <b>Jack Jewson</b>, University of Warwick</h2>
               <div class="accordion-content">
						
						<img src ="https://warwick.ac.uk/fac/sci/statistics/staff/research_students/jewson/jackjewsonacademic_cropped_resized.jpg" class="float-right" alt="Jack" height = "155">
						
						<p class="tab">
						<b> Speaker </b>: <a href = "https://warwick.ac.uk/fac/sci/statistics/staff/research_students/jewson/">Jack Jewson</a>, University of Warwick
						<br>

						<b> Title </b>: Doubly Robust Bayesian Inference for Non-Stationary Streaming Data with beta-Divergences
 						</p>
						
						<p class = "tab">

						<b>Abstract</b>: We present the first robust Bayesian Online Changepoint Detection algorithm through General Bayesian Inference (GBI) with beta-divergences. The resulting inference procedure is doubly robust for both the parameter and the changepoint (CP) posterior, with linear time and constant space complexity. We provide a construction for exponential models and demonstrate it on the Bayesian Linear Regression model. In so doing, we make two additional contributions: Firstly, we make GBI scalable using Structural Variational approximations that are exact as beta goes to 0. Secondly, we give a principled way of choosing the divergence parameter beta by minimizing expected predictive loss on-line. Reducing False Discovery Rates of CPs from over 90% to 0% on real world data, this offers the state of the art.
		       				</p>
						<p class="tab">
						Joint work with Jeremias Knoblauch and Theo Damoulas
						</p>      
				
						<p class= "tab">
							<b>Room and Time</b>: Enigma, 13h - 14h
						</p>
		    </div>
		    
		    <h2  class="accordion-header" >07 Mar 2019: <b>Edgar Dobriban</b>, University of Pennsylvania</h2>
               <div class="accordion-content">
		       
		       <img src ="https://www.amcs.upenn.edu/sites/default/files/styles/medium/public/Edgar%20Dobriban%20photo.jpg?itok=UGldwAal" class="float-right" alt="Edgar" height = "155">
						
						
					<p class="tab">
						<b> Speaker </b>: <a href = "https://statistics.wharton.upenn.edu/profile/dobriban/">Edgar Dobriban</a>, University of Pennsylvania
						<br>

						<b> Title </b>: How to deal with big data? Understanding large-scale distributed regression
 

						</p>
						
						<p class = "tab">

						<b>Abstract</b>:  Modern massive datasets pose an enormous computational burden to practitioners. Distributed computation has emerged as a universal approach to ease the burden: Datasets are partitioned over machines, which compute locally, and communicate short messages. Distributed data also arises due to privacy reasons, such as with medical databases. It is important to study how to do statistical inference and machine learning in a distributed setting.  In this talk, we present results about one-step parameter averaging in statistical linear models under data parallelism. We do linear regression on each machine, and take a weighted average of the parameters. How much do we lose compared to doing linear regression on the full data? Here we study the performance loss in estimation error, test error, and confidence interval length in high dimensions, where the number of parameters is comparable to the training data size. We discover several key phenomena. First, averaging is not optimal, and we find the exact performance loss. Second, different problems are affected differently by the distributed framework. Estimation error and confidence interval length increases a lot, while prediction error increases much less. These results match numerical simulations and a data analysis example. To derive these results, we rely on recent results from random matrix theory, where we also develop a new calculus of deterministic equivalents as a tool of broader interest.

						</p>      
				
						<p class= "tab">
							<b>Room and Time</b>: Augusta, 14h-15h
						</p>
		    </div>
		    
		    
	<h2  class="accordion-header" >28 Feb 2019: <b>Seok Young Hong</b>, University of Nottingham</h2>
               <div class="accordion-content">
						
					<p class="tab">
						<b> Speaker </b>: <a href = "https://sy-hong.net">Seok Young Hong</a>, University of Nottingham
						<br>

						<b> Title </b>: Nonparametric estimation of infinite order regression and its application to finance.
 

						</p>
						
						<p class = "tab">

						<b>Abstract</b>:  In this walk, we study nonparametric estimation of the infinite order regression with stationary and weakly dependent data. We propose a Nadaraya-Watson type estimator that operates with an infinite number of conditioning variables. We propose a bandwidth sequence that shrinks the effects of long lags, so the influence of all conditioning information is modelled in a natural and flexible way, and the econometric issues of omitted information bias and specification error are effectively handled. We establish the asymptotic properties of the estimator under a wide range of static and dynamic regressions framework, thereby allowing various kinds of conditioning variables to  be used. We establish pointwise/uniform consistency and the CLTs. We show that the convergence rates are at best logarithmic, and depend on the smoothness of the regression, the distribution of the marginal regressors and their dependence structure in a non-trivial way via the Lambert W function. As an application, the proposed theory is applied to investigate an important question in finance. We report some new empirical evidence on the dynamics of risk-return relation over time and its link with the macroeconomy, and also add supporting evidence for explaining some major puzzles in finance.


						</p>      
				
						<p class= "tab">
							<b>Room and Time</b>: Enigma, 14h-15h
						</p>
		    </div>	    
		    
		    		    
	<h2  class="accordion-header" >21 Feb 2019: <b>Weining Wang</b>, City, University of London</h2>
               <div class="accordion-content">
						
						<img src ="https://lehre.wiwi.hu-berlin.de/Professuren/quantitativ/statistik/members/personalpages/w2/smile.jpg" class="float-right" alt="Weining" height = "155">
						
						<p class="tab">
						<b> Speaker </b>: <a href = "https://sites.google.com/site/weiningwanghu/home">Weining Wang</a>, City, University of London
						<br>

						<b> Title </b>: LASSO-Driven Inference in Time and Space
 

						</p>
						
						<p class = "tab">

						<b>Abstract</b>: We consider the estimation and inference in a system of high-dimensional regression equations allowing for temporal and cross-sectional dependency in covariates and error processes, covering rather general forms of weak dependence. A sequence of large-scale regressions with LASSO is applied to reduce the dimensionality, and an overall penalty level is carefully chosen by a block multiplier bootstrap procedure to account for multiplicity of the equations and dependencies in the data. Correspondingly, oracle properties with a jointly selected tuning parameter are derived. We further provide high-quality de-biased simultaneous inference on the many target parameters of the system. We provide bootstrap consistency results of the test procedure, which are based on a general Bahadur representation for the $Z$-estimators with dependent data. Simulations demonstrate good performance of the proposed inference procedure.
Finally, we apply the method to quantify spillover effects of textual sentiment indices in a financial market and to test the connectedness among sectors.


						</p>      
				
						<p class= "tab">
							<b>Room and Time</b>: Augusta, 14h - 15h
						</p>
		    </div>
		    
		    	<h2  class="accordion-header" >19 Feb 2019: <b>Tengyao Wang</b>, University College London</h2>
               <div class="accordion-content">
						
						<img src ="http://www.homepages.ucl.ac.uk/~ucaktwa/img/profile_pic.jpg" class="float-right" alt="Tengyao" height = "155">
						
						<p class="tab">
						<b> Speaker </b>: <a href = "http://www.homepages.ucl.ac.uk/~ucaktwa/">Tengyao Wang</a>, University College London
						<br>

						<b> Title </b>: Isotonic regression in general dimensions
 

						</p>
						
						<p class = "tab">

						<b>Abstract</b>: We study the least squares regression function estimator over the class of real-valued functions on [0,1]^d that are increasing in each coordinate.  For uniformly bounded signals and with a fixed, cubic lattice design, we establish that the estimator achieves the minimax rate of order n^{-min(2/(d+2),1/d)} in the empirical L_2 loss, up to poly-logarithmic factors.  Further, we prove a sharp oracle inequality, which reveals in particular that when the true regression function is piecewise constant on k hyperrectangles, the least squares estimator enjoys a faster, adaptive rate of convergence of (k/n)^{min(1,2/d)}, again up to poly-logarithmic factors.  Previous results are confined to the case d \leq 2.  Finally, we establish corresponding bounds (which are new even in the case d=2) in the more challenging random design setting.  There are two surprising features of these results: first, they demonstrate that it is possible for a global empirical risk minimisation procedure to be rate optimal up to poly-logarithmic factors even when the corresponding entropy integral for the function class diverges rapidly; second, they indicate that the adaptation rate for shape-constrained estimators can be strictly worse than the parametric rate.

						</p>      
				
						<p class= "tab">
							<b>Room and Time</b>: Augusta, 11h-12h
						</p>
		    </div>
		    
		 <h2  class="accordion-header" >14 Feb 2019: <b>Lyudmila Grigoryeva</b>, Universität Konstanz</h2>
               <div class="accordion-content">
			<img src ="http://www.grigoryeva.info/wp-content/uploads/2014/01/autoColor.jpg" class="float-right" alt="Lyudmila" height = "155">
		
						
						<p class="tab">
							<b> Speaker </b>: <a href="http://www.grigoryeva.info">Lyudmila Grigoryeva</a>, Universität Konstanz
						<br>

						<b> Title </b>: Reservoir Computing (RC). Part II: Applicability of RC systems with unbounded inputs and their empirical performance.
 

						</p>
						
						<p class = "tab">

						<b>Abstract</b>: We show an extension of the results presented in the previous talk to unbounded inputs which is a particularly natural setup for stochastic signals. We complement the universality results with an empirical assessment of performance in applications to time series forecasting. In particular, we focus on predicting financial realized covariance matrices which is a question of much importance for practitioners and researchers. We examine the empirical performance of RC in comparison with many conventional state-of-the-art econometric models for various realized covariance estimators, periods, and dimensions. We show that universal RC families consistently demonstrate superior predictive ability for various empirical designs. We conclude the presentation outlining the work in progress and future lines of research.

						</p>      
				
						<p class= "tab">
							<b>Room and Time</b>: Augusta, 14:40h-15:15h
						</p>
		    </div> 
		    
		<h2  class="accordion-header" >14 Feb 2019: <b>Juan-Pablo Ortega</b>, Universität St. Gallen + Centre National de Recherche Scientifique</h2>
               <div class="accordion-content">
						
		       <img src ="https://juan-pablo-ortega.com/Home_files/photoSG.jpg" class="float-right" alt="Juan" height = "155">
						
						
						<p class="tab">
							<b> Speaker </b>: <a href="https://juan-pablo-ortega.com/">Juan-Pablo Ortega</a>, U St Gallen + CNRS
						<br>

						<b> Title </b>: Reservoir Computing (RC). Part I: RC paradigm and universal approximation properties of RC systems.
 
						</p>
						
						<p class = "tab">

						<b>Abstract</b>: A relatively recent family of dynamic machine learning paradigms known collectively as reservoir computing is presented which is capable of unprecedented performances in the forecasting of deterministic and stochastic processes. We then focus on the universal approximation properties of the most widely used families of reservoir computers in applications. These results are a much awaited generalization to the dynamic
context of previous well-known static results obtained in the context of neural networks. The universal approximation properties with respect to L^\infty  and L^p -type criteria of three important families of reservoir computers with stochastic discrete-time semi-infinite inputs are shown. First, it is proved that linear reservoir systems with either polynomial or neural network readout maps are universal. More importantly, it is proved that the same property holds for two families with linear readouts, namely, state-afine systems and echo state networks. The linearity in the readouts is a key feature in supervised machine learning applications. It guarantees that these systems can be used in high-dimensional situations and in the presence of large datasets. 
						</p>      
				
						<p class= "tab">
							<b>Room and Time</b>: Augusta, 14h - 14:35h
						</p>
		    </div> 
		    
		
		
		<h2  class="accordion-header" >07 Feb 2019: <b>Xinghao Qiao</b>, London School of Economics</h2>
               <div class="accordion-content">
						
						<img src ="http://personal.lse.ac.uk/qiaox/qiao.links/images/Xinghao_meitu.jpg" class="float-right" alt="Xinghao" height = "155">
						
						<p class="tab">
						<b> Speaker </b>: <a href = "http://personal.lse.ac.uk/qiaox/">Xinghao Qiao</a>, London School of Economics
						<br>

						<b> Title </b>: A General Theory for Large-Scale Curve Time Series via Functional Stability Measure
 

						</p>
						
						<p class = "tab">

						<b>Abstract</b>: Modelling a large bundle of curves arises in a broad spectrum of real applications. However, 
							existing literature relies primarily on the critical assumption of independent curve observations. 
							In this talk, we provide a general theory for large-scale Gaussian curve time series, where the temporal 
							and cross-sectional dependence across multiple curve observations exist and the number of functional variables, 
							p,  may be large relative to the number of observations, n.  We propose a novel functional stability measure 
							for multivariate stationary processes based on their spectral properties and use it to establish some useful 
							concentration bounds on the sample covariance matrix function. These concentration bounds serve as a 
							fundamental tool for further theoretical analysis, in particular, for deriving nonasymptotic upper bounds 
							on the errors of the regularized estimates in high dimensional settings. As functional principle component 
							analysis (FPCA) is one of the key techniques to handle functional data, we also investigate the concentration 
							properties of the relevant estimated terms under a FPCA framework. To illustrate with an important application,
							we consider vector functional autoregressive models and develop a regularization approach to estimate 
							autoregressive coefficient functions under the sparsity constraint. Using our derived nonasymptotic results,
							we investigate the theoretical properties of the regularized estimate in a "large p,  small n" regime. The 
							finite sample performance of the proposed method is examined through simulation studies.
						</p>      
				
						<p class= "tab">
							<b>Room and Time</b>: Augusta, 14h-15h
						</p>
		    </div>

		
		<h2  class="accordion-header" >05 Feb 2019: <b>Clifford Lam</b>, London School of Economics</h2>
                        <div class="accordion-content">
						
						<img src ="http://stats.lse.ac.uk/lam/index_files/image003.jpg" class="float-right" alt="Clifford" height = "155">
						
						<p class="tab">
						<b> Speaker </b>: <a href = "http://stats.lse.ac.uk/lam/">Clifford Lam</a>, London School of Economics
						<br>

						<b> Title </b>: Testing for High-Dimensional White Noise
 

						</p>
						
						<p class = "tab">

						<b>Abstract</b>: Testing for white noise is a classical yet important problem in statistics, especially for diagnostic checks in time series modeling and linear regression. For high-dimensional time series in the sense that the dimension p is large in relation to the sample size T, the popular omnibus tests including the multivariate Hosking and Li-McLeod tests are extremely conservative, leading to substantial power loss. To develop more relevant tests for high-dimensional cases, we propose a portmanteau-type test statistic which is the sum of squared singular values of the first q lagged sample autocovariance matrices. It, therefore, encapsulates all the serial correlations (upto the time lag q)
within and across all component series. Using the tools from random matrix theory and assuming both p and T diverge to infinity, we derive the asymptotic normality of the test statistic under both the null and a specific VMA(1) alternative hypothesis. As the actual implementation of the test requires the knowledge of three characteristic constants of the population cross-sectional covariance matrix and the value of the fourth moment of the standardized innovations, non-trivial estimations are proposed for these parameters and their integration leads to a practically usable test. Extensive simulation confirms the excellent finite-sample performance of the new test with accurate size and satisfactory power for a large range of finite (p, T) combinations, therefore ensuring wide applicability in practice. We also talk about alternative tests using maximal correlations. Results can be used even for nonlinear processes and non-iid data.
						</p>      
				
						<p class= "tab">
							<b>Room and Time</b>: Augusta, 14h-15h
						</p>
		    </div>
		
		
	<h2  class="accordion-header" >29 Jan 2019: <b>Xiaowen Dong</b>, University of Oxford</h2>
                        <div class="accordion-content">
						
						<img src ="http://web.media.mit.edu/~xdong/photo3.jpg" class="float-right" alt="Xiaowen" height = "155">
						
						<p class="tab">
						<b> Speaker </b>: <a href = "http://web.media.mit.edu/~xdong/">Xiaowen Dong</a>, University of Oxford
						<br>

						<b> Title </b>: Learning Quadratic Games on Networks
 

						</p>
						
						<p class = "tab">

						<b>Abstract</b>: Individuals, or organisations, cooperate with or compete against one another in 
a wide range of practical situations. In the economics literature, such strategic 
interactions are often modelled as games played on networks, where an 
individual's payoff depends not only on her action but also that of her neighbours. 
The current literature has largely focused on analysing the characteristics of 
network games in the scenario where the structure of the network, which is 
represented by a graph, is known beforehand. It is often the case, however, 
that the actions of the players are readily observable while the underlying 
interaction network remains hidden. In this talk, I will introduce two novel 
frameworks for learning, from the observations on individual actions, network 
games with linear-quadratic payoffs, and in particular the structure of the 
interaction network. Both frameworks are based on the Nash equilibrium of 
such games and involve solving a joint optimisation problem for the graph 
structure and the individual marginal benefits. We test the proposed frameworks 
on both synthetic and real world examples, and show that they can effectively 
and more accurately learn the games than the baselines. The proposed 
approach has both theoretical and practical implications for understanding 
strategic interactions in a network environment.
						</p>      
				
						<p class= "tab">
							<b>Room and Time</b>: TBA, 14h - 15h
						</p>
		    </div>
		</div>
	</div>

</div>
	
<!-- Start of StatCounter Code for Default Guide -->
<script type="text/javascript">
var sc_project=11444464; 
var sc_invisible=1; 
var sc_security="a64628fa"; 
var scJsHost = (("https:" == document.location.protocol) ?
"https://secure." : "http://www.");
document.write("<sc"+"ript type='text/javascript' src='" +
scJsHost+
"statcounter.com/counter/counter.js'></"+"script>");
</script>
<noscript><div class="statcounter"><a title="web analytics"
href="http://statcounter.com/" target="_blank"><img
class="statcounter"
src="//c.statcounter.com/11444464/0/a64628fa/1/" alt="web
analytics"></a></div></noscript>
<!-- End of StatCounter Code for Default Guide -->

</body>
</html>
